##############################
## Hazelcast Configurations ##
##############################
## For Hazelcast configurations, it is recommended to be done in the standard externalized XML
## by providing a configuration XML path at 'spring.hazelcast.config', 
## and specifying @SpringBootApplication(exclude = {HazelcastAutoConfiguration.class}). This is important to allow proper group joining.
spring.hazelcast.config=hazelcast.xml

#########################
## REST Configurations ##
#########################
server.port=8081

## Range offset till which port will be incremented in order to find an available port. Default 100.
#server.port-offset=

server.context-path=/ticker

## Unique identifier for this running instance in the cluster.
#instance.id=node-1

##############################
## Scheduler Configurations ##
##############################

## No of scheduler threads. Default 4
scheduler.threadPoolSize=10

## Await in seconds while terminating scheduler threads. Default 0
scheduler.awaitTerminationSeconds=1

## There are some configurations to be provided in AbstractScheduledTask subclasses. Check javadocs for the same.

##############################
## Messaging Configurations ##
##############################

## -- DEPRECATED --
## Publisher concurrency. By concurrency we set the degree of parallelism, and not the number of threads. Default 1.
#pub.concurrency=4
## Publisher timeout in millis, for synchronous message posting (add/append). Default 1000.
#pub.sync.timeout=1000
## ----------------

## Whether to clear all pending entries on startup, or just the locally owned entries. Default false (local entries only). This
## can be needed in testing scenario only
container.clear_all_pending=true

## Whether to remove a consumed entry synchronously from Hazelcast. Default false, to be removed in an asynchronous manner.
#container.remove_entry_immediate=true

## Whether to hold an exclusive lock before start processing an entry. This will introduce somewhat stronger consistency in
## a distributed processing environment, at the cost of an added complexity. What consistency? Say an entry has been completed
## processing in a node, but yet to be committed (removed from distributed map) and this node goes down. The entry primary will be assigned
## to another node on account of partition migration, it will get consumed again. Default false.
#container.process_entry_exclusive:false

## -----------------------------------------------------------------
## RingBuffer backed implementation. EXPERIMENTAL AND NOT ENABLED ##
## The basic challenge in using a ring buffer backed queue, is the bounded nature of it due to which it might not be very scalable.

## Circular array size for the ring buffer container, if used. This has to be power of 2, as per LMAX Disruptor library.
#container.ringbuff.size=2

## Core threads working in the fork-join pool, for ring buffer container, if used.
#container.ringbuff.threads=8
## -----------------------------------------------------------------

############################
## Logger Configurations ###
############################
#logging.pattern.console=%d{ddMMMyy HH:mm:ss} %-5level [%thread] %.-40logger{0} - %msg%n
#logging.level.io.netty=DEBUG
#logging.level.com.prototype.cmq=DEBUG
#logging.level.com.datastax.driver=DEBUG
#logging.level.org.springframework.data.cassandra=DEBUG
logging.level.org.springframework.web=WARN
logging.level.org.springframework.boot=WARN
#logging.level.org.springframework.data=DEBUG

#logging.level.org.reactivetechnologies.ticker=DEBUG

logging.level.com.hazelcast.internal.cluster=INFO
logging.level.com.hazelcast.cluster=INFO
logging.level.com.hazelcast.system=INFO
logging.level.com.hazelcast=WARN
